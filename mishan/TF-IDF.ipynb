{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5959420",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "tf(w)*idf(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de03f64",
   "metadata": {},
   "source": [
    "tf(w) = (no. of times word appeared in the document) / (no. of words in the document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93aaf1c",
   "metadata": {},
   "source": [
    "idf(w) = log(no. of documents / no. of documents that contains the word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651b162",
   "metadata": {},
   "source": [
    "# countvectorizer for word frequency in diff. array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb5d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b70857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docA = [\"i use my pen to write my book\"]\n",
    "docB = [\"i use my spectacles to read my book\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d99a1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer1 = TfidfVectorizer()\n",
    "vectorizer2 = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f5a3755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.3333333333333333\n",
      "  (0, 5)\t0.3333333333333333\n",
      "  (0, 3)\t0.3333333333333333\n",
      "  (0, 2)\t0.3333333333333333\n",
      "  (0, 1)\t0.6666666666666666\n",
      "  (0, 4)\t0.3333333333333333\n",
      "gap\n",
      "  (0, 0)\t0.3333333333333333\n",
      "  (0, 2)\t0.3333333333333333\n",
      "  (0, 4)\t0.3333333333333333\n",
      "  (0, 3)\t0.3333333333333333\n",
      "  (0, 1)\t0.6666666666666666\n",
      "  (0, 5)\t0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "dict1 = vectorizer1.fit_transform(docA)\n",
    "dict2 = vectorizer2.fit_transform(docB)\n",
    "print(dict1)\n",
    "print(\"gap\")\n",
    "print(dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2980a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333]]\n",
      "[[0.33333333 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "print(dict1.todense())\n",
    "print(dict1.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "404f0929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'my', 'pen', 'to', 'use', 'write']\n",
      "[[0.33333333 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333]]\n",
      "['book', 'my', 'read', 'spectacles', 'to', 'use']\n",
      "[[0.33333333 0.66666667 0.33333333 0.33333333 0.33333333 0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer1.get_feature_names())\n",
    "print(dict1.todense())\n",
    "print(vectorizer2.get_feature_names())\n",
    "print(dict1.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac345a6",
   "metadata": {},
   "source": [
    "# countvectorizer for word frequency in same array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a17b765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the library\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a30b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"i use my pen to write my book\",\n",
    "        \"i use my spectacles to read my book\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eff6d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdf1a6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.30218977576862155\n",
      "  (0, 7)\t0.42471718586982765\n",
      "  (0, 5)\t0.30218977576862155\n",
      "  (0, 2)\t0.42471718586982765\n",
      "  (0, 1)\t0.6043795515372431\n",
      "  (0, 6)\t0.30218977576862155\n",
      "  (1, 3)\t0.42471718586982765\n",
      "  (1, 4)\t0.42471718586982765\n",
      "  (1, 0)\t0.30218977576862155\n",
      "  (1, 5)\t0.30218977576862155\n",
      "  (1, 1)\t0.6043795515372431\n",
      "  (1, 6)\t0.30218977576862155\n"
     ]
    }
   ],
   "source": [
    "dictionary = vectorizer.fit_transform(docs)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1e32e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'my', 'pen', 'read', 'spectacles', 'to', 'use', 'write']\n",
      "[[0.30218978 0.60437955 0.42471719 0.         0.         0.30218978\n",
      "  0.30218978 0.42471719]\n",
      " [0.30218978 0.60437955 0.         0.42471719 0.42471719 0.30218978\n",
      "  0.30218978 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "print(dictionary.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454f4f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
